{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"/home/martin/Documents/Exjobb/eed/.data/augmented_data/train/augmented_data_SE_recorded_noise_0_train.parquet\"\n",
    "\n",
    "# Load the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "#df[\"acc_averaged\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts().sort_index().plot(kind='bar')\n",
    "print(df['label'].value_counts().sort_index())\n",
    "fixation_percentage = round((df['label'].value_counts().sort_index()[1] / df['label'].value_counts().sum()) * 100, 2)\n",
    "saccade_percentage = round((df['label'].value_counts().sort_index()[2] / df['label'].value_counts().sum()) * 100, 2)\n",
    "\n",
    "print(\"Percentage of fixations: \", fixation_percentage, \"%\")\n",
    "print(\"Percentage of saccades: \", saccade_percentage, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of labels when only looking at fixations and saccades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_dropped = df.dropna()\n",
    "print(df)\n",
    "# Reassign labels 3 and 4 to label 1\n",
    "df_na_dropped.loc[:, 'label'] = df_na_dropped['label'].replace({3: 1, 4: 1})\n",
    "\n",
    "# Remove samples with label 1 and 5\n",
    "df_na_dropped = df_na_dropped[(df_na_dropped['label'] != 5) & (df_na_dropped['label'] != 0)]\n",
    "\n",
    "df_na_dropped['label'].value_counts().sort_index().plot(kind='bar')\n",
    "print(df_na_dropped['label'].value_counts().sort_index())\n",
    "fixation_percentage = round((df_na_dropped['label'].value_counts().sort_index()[1] / df_na_dropped['label'].value_counts().sum()) * 100, 2)\n",
    "saccade_percentage = round((df_na_dropped['label'].value_counts().sort_index()[2] / df_na_dropped['label'].value_counts().sum()) * 100, 2)\n",
    "\n",
    "print(\"Percentage of fixations: \", fixation_percentage, \"%\")\n",
    "print(\"Percentage of saccades: \", saccade_percentage, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_na_dropped.drop(columns=['label',\"x\", \"y\",\"t\", \"status\", \"file_index\", \"file_name\"])\n",
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA with 2 components\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Perform PCA on the normalized features\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(normalized_features)\n",
    "\n",
    "# Create a scatter plot of the PCA components, color-coded by labels\n",
    "plt.scatter(pca_components[:, 0], pca_components[:, 1], c=df_na_dropped[\"label\"],alpha=0.5)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA Components')\n",
    "plt.colorbar(label='Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap as umap\n",
    "\n",
    "# Sample 1000 samples\n",
    "sample_df = df_na_dropped.sample(n=10000, random_state=42)\n",
    "\n",
    "# Perform UMAP with 2 components on the sampled data\n",
    "umap_components = umap.UMAP(n_components=2).fit_transform(sample_df[features.columns])\n",
    "\n",
    "# Create a scatter plot of the UMAP components, color-coded by labels\n",
    "plt.scatter(umap_components[:, 0], umap_components[:, 1], c=sample_df[\"label\"])\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.title('UMAP Components')\n",
    "plt.colorbar(label='Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "sample_df = df_na_dropped.sample(n=10000, random_state=42)\n",
    "normalized_features = scaler.fit_transform(sample_df[features.columns])\n",
    "\n",
    "\n",
    "# Perform t-SNE with 2 components on the normalized features\n",
    "tsne_components = TSNE(n_components=2).fit_transform(normalized_features)\n",
    "\n",
    "# Create a scatter plot of the t-SNE components, color-coded by labels\n",
    "plt.scatter(tsne_components[:, 0], tsne_components[:, 1], c=sample_df[\"label\"])\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('t-SNE Components')\n",
    "plt.colorbar(label='Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sns pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "# Concatenate the features and labels into a single DataFrame\n",
    "df_concat = pd.concat([features, df_na_dropped['label']], axis=1)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(df_concat.iloc[:, :-1])\n",
    "\n",
    "# Create a DataFrame with the normalized features and label\n",
    "normalized_df = pd.DataFrame(normalized_features, columns=df_concat.columns[:-1])\n",
    "normalized_df['label'] = df_concat['label']\n",
    "\n",
    "# Randomly extract a subset of the data\n",
    "subset_df = normalized_df.sample(n = 5000, random_state=42)\n",
    "\n",
    "# Create a pairplot\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    sns.pairplot(subset_df, hue='label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print correlations between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = features.corr()\n",
    "correlation_ranking = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "correlation_ranking = correlation_ranking[correlation_ranking != 1]\n",
    "pd.options.display.max_rows = 4000\n",
    "print(correlation_ranking[1:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distributions of different features for saccades and fixations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the list of feature column names\n",
    "feature_columns = list(features.columns)\n",
    "\n",
    "# Set the number of rows and columns for the subplots\n",
    "num_rows = len(feature_columns)\n",
    "num_cols = 1\n",
    "\n",
    "# Create subplots for each feature\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 5*num_rows))\n",
    "\n",
    "# Iterate over the feature columns\n",
    "for i, feature in enumerate(feature_columns):\n",
    "    # Extract samples where the label is 1\n",
    "    label_1_samples = df_na_dropped[df_na_dropped['label'] == 1]\n",
    "\n",
    "    # Extract samples where the label is 2\n",
    "    label_2_samples = df_na_dropped[df_na_dropped['label'] == 2]\n",
    "\n",
    "    # Plot the distribution of the feature for label 1 samples\n",
    "    axs[i].hist(label_1_samples[feature], bins=100, alpha=0.5, label='Fixations', density=True)\n",
    "    axs[i].hist(label_2_samples[feature], bins=100, alpha=0.5, label='Saccades', density=True)\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].set_ylabel('Normalized Frequency')\n",
    "    axs[i].set_title(f'Distribution of {feature} for Fixations and Saccade Samples')\n",
    "    #axs[i].set_yscale('log')\n",
    "    axs[i].legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable importance with random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Fit the classifier to your data\n",
    "rf_classifier.fit(features, df_na_dropped['label'])\n",
    "\n",
    "# Get the feature importances\n",
    "importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a dataframe with feature names and importances\n",
    "feature_importances = pd.DataFrame({'Feature': features.columns, 'Importance': importances})\n",
    "\n",
    "# Sort the dataframe by importance in descending order\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(feature_importances)\n",
    "plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_na_dropped.select_dtypes(include=np.number))\n",
    "\n",
    "# Create a new DataFrame with the scaled features\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=df_na_dropped.select_dtypes(include=np.number).columns)\n",
    "\n",
    "# Create a scatter plot for each numeric column in df_na_dropped\n",
    "fig = go.Figure()\n",
    "xx = np.array(range(len(df_na_dropped['x'])))\n",
    "for column in df_na_dropped.select_dtypes(include=np.number).columns:\n",
    "    fig.add_trace(go.Scatter(x=xx, y=df_scaled[column], name=column))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Plot of Numeric Features (Scaled)',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Scaled Value',\n",
    "                  showlegend=True)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_na_dropped.select_dtypes(include=np.number))\n",
    "\n",
    "# Create a new DataFrame with the scaled features\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=df_na_dropped.select_dtypes(include=np.number).columns)\n",
    "\n",
    "# Create subplots with shared x-axis\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "# Add a trace for each numeric column in df_na_dropped to the first subplot\n",
    "xx = np.array(range(len(df_na_dropped['x'])))\n",
    "for column in df_na_dropped.select_dtypes(include=np.number).columns:\n",
    "    fig.add_trace(go.Scatter(x=xx, y=df_scaled[column], name=column), row=1, col=1)\n",
    "\n",
    "# Add a trace for each numeric column in df_na_dropped to the second subplot\n",
    "for column in df_na_dropped.select_dtypes(include=np.number).columns:\n",
    "    fig.add_trace(go.Scatter(x=xx, y=df_scaled[column], name=column), row=2, col=1)\n",
    "\n",
    "# Update layout for the first subplot\n",
    "fig.update_layout(title='Plot of Numeric Features (Scaled)',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Scaled Value', \n",
    "                  showlegend=True)\n",
    "\n",
    "# Update layout for the second subplot\n",
    "fig.update_layout(xaxis2=dict(matches='x'),\n",
    "                  yaxis2=dict(showticklabels=False),\n",
    "                  xaxis3=dict(matches='x'),\n",
    "                  yaxis3=dict(showticklabels=False))\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
